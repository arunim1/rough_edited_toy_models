# -*- coding: utf-8 -*-
"""rough_edited_toy_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t2OKIsoAk5kIJbWeeJTZbMb0MHYL51nc

# Toy Models of Superposition

This notebook includes the toy model training framework used to generate most of the results in the "Toy Models of Superposition" paper.

The main useful improvement over a basic PyTorch tiny autoencoder is the ability to batch train many models with varying sparsity at once, which is much more efficient than training them one at a time.

This notebook is designed to run in Google Colab's Python 3.7 environment.
"""

!pip install einops
# !pip install kaleido

import torch
from torch import nn
from torch.nn import functional as F
from torch.distributions import MultivariateNormal

from typing import Optional

from dataclasses import dataclass, replace
import numpy as np
import einops

# from scipy.stats import multivariate_normal

from tqdm.notebook import trange

import time
import pandas as pd

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

import matplotlib.pyplot as plt

# import kaleido

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
@dataclass
class Config:
  n_features: int
  n_hidden: int

  # We optimize n_instances models in a single training loop
  # to let us sweep over sparsity or importance curves 
  # efficiently.

  # We could potentially use torch.vmap instead.
  n_instances: int 
 
class Model(nn.Module):
  def __init__(self, 
               config, 
               feature_probability: Optional[torch.Tensor] = None,
               importance: Optional[torch.Tensor] = None, 
               depend: Optional[int] = None,  # Arunim's edit here
               cov: Optional[torch.Tensor] = None,  # Arunim's edit here
               device='cuda'):
    super().__init__()
    self.config = config
    self.W = nn.Parameter(torch.empty((config.n_instances, config.n_features, config.n_hidden), device=device))
    nn.init.xavier_normal_(self.W)
    self.b_final = nn.Parameter(torch.zeros((config.n_instances, config.n_features), device=device))

    if feature_probability is None:
      feature_probability = torch.ones(())
    self.feature_probability = feature_probability.to(device)
    if importance is None:
      importance = torch.ones(())
    self.importance = importance.to(device)
    if depend is None:
      depend = -1
    self.depend = depend
    if cov is None:
      cov = torch.eye(config.n_features, device=device)
    self.cov = cov

  def forward(self, features):
    # features: [..., instance, n_features]
    # W: [instance, n_features, n_hidden]
    hidden = torch.einsum("...if,ifh->...ih", features, self.W)
    out = torch.einsum("...ih,ifh->...if", hidden, self.W)
    out = out + self.b_final
    out = F.relu(out)
    return out

  def generate_batch(self, n_batch):
    if self.depend == 0:  # Input features where the 2nd most important feature is sampled from a small (std = 0.05) distribution around most important feature's value
      # The values are dependent, but their existence is independent (solely depends on overall model's sparsity)
      feat = torch.rand((n_batch, self.config.n_instances, self.config.n_features), device='cpu')  # creating a random tensor of size n_batch x n_instances x features. these are a bunch of instances of sets of features. they are inputs. 

      indices = torch.index_select(feat, 2, torch.tensor([0]).to('cpu'))  # features on which others are dependent ("f1s dependent on f0s")
      indices_reshaped = indices.reshape((n_batch, self.config.n_instances))
      feat[:, :, 1] = torch.normal(mean=indices_reshaped, std=0.05)  # setting dependent features to random values similar to their dependencies

      feat = feat.to(self.W.device)

      batch = torch.where(
          torch.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device) <= self.feature_probability,
          feat,
          torch.zeros((), device=self.W.device),
      )  # creates batch tensor with dims same as above, replacing all vals <= feature_probability with the random val from feat, randomly removing ~(1-feature_probability) of the inputs and replacing them with zero.
      return batch

    elif self.depend == 1: # Input features where each feature is selected from a multi-dim gaussian that has been mapped to [0, 1] with a CDF. These values are still independent given the diagonal cov.
      # The values are independent, and their existence is independent (solely depends on overall model's sparsity)

      # create covariance matrix, same for all, and mean vectors/1d tensors. 
      mean = torch.zeros(5, device='cpu')
      cov = torch.eye(5, device='cpu')  # if ID / diagonal matrix, they are independent... careful. I think it also has to be symmetric by def. 
      
      # Create the distribution object defined by covariance matrix
      dist = multivariate_normal(mean=mean, cov=cov)

      # Get the samples
      num_samples = n_batch * self.config.n_instances * self.config.n_features
      
      samples = dist.rvs(size=num_samples)
      # samples = samples.reshape(n_batch, self.config.n_instances, self.config.n_features, self.config.n_features)  # b batches of i instances of f-D points

      # Compute the CDF for each tensor
#       %timeit  feat = torch.from_numpy(dist.cdf(samples))
      
      # feat = feat.to(self.W.device)

      # make these [0, 1] (solve for variance -> std -> divide by std -> run through CDF, this is standard practice) or worst case (divide by abs value max val)
      # create batch inputs based on distributions defined by covariance matrix

      batch = torch.where(
          torch.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device) <= self.feature_probability,
          feat.to(self.W.device).reshape(n_batch, self.config.n_instances, self.config.n_features),
          torch.zeros((), device=self.W.device),
      )  # creates batch tensor with dims same as above, replacing all vals <= feature_probability with the random val from feat, randomly removing ~(1-feature_probability) of the inputs and replacing them with zero.
      return batch.float()
      
    elif self.depend == 2:  # Input features where each feature is selected from a multi-dim gaussian that has been mapped to [0, 1] by dividing by abs().max(). These values are still independent given the diagonal cov.
      # The values are independent, and their existence is independent (solely depends on overall model's sparsity)

      mean = torch.zeros(self.config.n_features, device=self.W.device)
      num_samples = n_batch * self.config.n_instances

      dist = MultivariateNormal(mean, self.cov)
      
      samples = dist.sample((num_samples,)).to(self.W.device)
      samples = samples.abs()
      
      feat = samples / samples.max()

      feat = feat.to(self.W.device).reshape(n_batch, self.config.n_instances, self.config.n_features)
      batch = torch.where(
          torch.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device) <= self.feature_probability,
          feat,
          torch.zeros((), device=self.W.device),
      ) 
      return batch

    # properly created batch inputs using multivariate normal distribution
    elif self.depend == 3:
        mean = torch.zeros(self.config.n_features, device=self.W.device)
        num_samples = self.config.n_instances

        diag = torch.diagonal(self.cov).to(self.W.device)
        sigmas = torch.sqrt(diag).to(self.W.device)

        dists = torch.distributions.Normal(torch.zeros(self.config.n_features).to(self.W.device), sigmas)
        limits = dists.icdf(((1 + self.feature_probability) / 2) - 0.0000001).to(self.W.device)  # this is working, correct dimensions, small number to avoid infs

        dist = MultivariateNormal(mean, self.cov.to(self.W.device))

        feat = dist.sample((n_batch, num_samples,)).to(self.W.device)
        feat = feat.abs().to(self.W.device)

        batch = torch.where(
            feat <= (torch.ones((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device) * limits.to(self.W.device)).to(self.W.device),
            feat,
            torch.zeros((), device=self.W.device),
        )

        batch = batch / (torch.ones((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device) * limits.to(self.W.device)).to(self.W.device)
        batch = batch.to(self.W.device).reshape(n_batch, self.config.n_instances, self.config.n_features)

        return batch

    # BETA DISTRIBUTION HERE based on some parameters...
    elif self.depend == 4:
        return

    else:
      feat = torch.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device)  # creating a random tensor of size n_batch x n_instances x features. these are a bunch of instances of sets of features. they are inputs. 
      batch = torch.where(
          torch.rand((n_batch, self.config.n_instances, self.config.n_features), device=self.W.device) <= self.feature_probability,
          feat,
          torch.zeros((), device=self.W.device),
      )  # creates batch tensor with dims same as above, replacing all vals <= feature_probability with the random val from feat, randomly removing ~(1-feature_probability) of the inputs and replacing them with zero.
      return batch

def linear_lr(step, steps):
  return (1 - (step / steps))

def constant_lr(*_):
  return 1.0

def cosine_decay_lr(step, steps):
  return np.cos(0.5 * np.pi * step / (steps - 1))

def optimize(model, 
             render=False, 
             n_batch=1024,
             steps=10_000,
             print_freq=100,
             lr=1e-3,
             lr_scale=constant_lr,
             hooks=[]):
  cfg = model.config

  opt = torch.optim.AdamW(list(model.parameters()), lr=lr)

  start = time.time()
  with trange(steps) as t:
    for step in t:
      step_lr = lr * lr_scale(step, steps)
      for group in opt.param_groups:
        group['lr'] = step_lr
      opt.zero_grad(set_to_none=True)
      batch = model.generate_batch(n_batch)
      out = model(batch)
      error = (model.importance*(batch.abs() - out)**2)
      loss = einops.reduce(error, 'b i f -> i', 'mean').sum()
      loss.backward()
      opt.step()
    
      if hooks:
        hook_data = dict(model=model,
                         step=step, 
                         opt=opt,
                         error=error,
                         loss=loss,
                         lr=step_lr)
        for h in hooks:
          h(hook_data)
      if step % print_freq == 0 or (step + 1 == steps):
        t.set_postfix(
            loss=loss.item() / cfg.n_instances,
            lr=step_lr,
        )

if torch.cuda.is_available():
  DEVICE = 'cuda'
else:
  DEVICE = 'cpu'

"""# Making Covariance Matrices"""

def rand_cov(config):
    cfg = config
    # constructing a random correlation matrix
    rand = torch.randn(cfg.n_features, cfg.n_features)
    rand = torch.where(
        torch.rand((cfg.n_features, cfg.n_features)) >= 0.8,
        rand,
        torch.zeros(()),
    )
    rand = rand + torch.eye(cfg.n_features)
    cov = rand @ rand.t()
    return cov

def not_rand_cov(config):
    cfg = config
    # constructing a random correlation matrix
    r = torch.rand(1)  # 0 < r < 1
    r = 0
    cov = torch.ones(cfg.n_features, cfg.n_features) * r
    cov.fill_diagonal_(1)
    return cov

"""# Introduction Figure

Reproducing a version of the figure from the introduction, although with a slightly different version of the code.
"""

config = Config(
    n_features = 5,
    n_hidden = 2,
    n_instances = 10,
)

model = Model(
    config=config,
    device=DEVICE,
    # Exponential feature importance curve from 1 to 1/100
    importance = (0.9**torch.arange(config.n_features))[None, :],
    # Sweep feature frequency across the instances from 1 (fully dense) to 1/20
    feature_probability = (20 ** -torch.linspace(0, 1, config.n_instances))[:, None],
    depend = 3,  # correct batch generation
    cov=not_rand_cov(config)
)

optimize(model)

def plot_intro_diagram(model):
  from matplotlib import colors  as mcolors
  from matplotlib import collections  as mc
  cfg = model.config
  WA = model.W.detach()
  N = len(WA[:,0])
  sel = range(config.n_instances) # can be used to highlight specific sparsity levels
  plt.rcParams["axes.prop_cycle"] = plt.cycler("color", plt.cm.viridis(model.importance[0].cpu().numpy()))
  plt.rcParams['figure.dpi'] = 200
  fig, axs = plt.subplots(1,len(sel), figsize=(2*len(sel),2))
  for i, ax in zip(sel, axs):
      W = WA[i].cpu().detach().numpy()
      colors = [mcolors.to_rgba(c)
            for c in plt.rcParams['axes.prop_cycle'].by_key()['color']]
      ax.scatter(W[:,0], W[:,1], c=colors[0:len(W[:,0])])
      ax.set_aspect('equal')
      ax.add_collection(mc.LineCollection(np.stack((np.zeros_like(W),W), axis=1), colors=colors))
      
      z = 1.5
      ax.set_facecolor('#FCFBF8')
      ax.set_xlim((-z,z))
      ax.set_ylim((-z,z))
      ax.tick_params(left = True, right = False , labelleft = False ,
                  labelbottom = False, bottom = True)
      for spine in ['top', 'right']:
          ax.spines[spine].set_visible(False)
      for spine in ['bottom','left']:
          ax.spines[spine].set_position('center')
  images_dir = (f'/content/drive/My Drive/Colab Notebooks/edited_toy_images')
  curr = time.time()
  plt.savefig(f"{images_dir}/{curr}.png")  
  plt.show()

plot_intro_diagram(model)

"""# Visualizing features across varying sparsity"""

config = Config(
    n_features = 100,
    n_hidden = 20,
    n_instances = 20,
)

model = Model(
    config=config,
    device=DEVICE,
    # Exponential feature importance curve from 1 to 1/100
    importance = (100 ** -torch.linspace(0, 1, config.n_features))[None, :],
    # Sweep feature frequency across the instances from 1 (fully dense) to 1/20
    feature_probability = (20 ** -torch.linspace(0, 1, config.n_instances))[:, None],
    depend = 2
)

optimize(model)

def render_features(model, which=np.s_[:]):
  cfg = model.config
  W = model.W.detach()
  W_norm = W / (1e-5 + torch.linalg.norm(W, 2, dim=-1, keepdim=True))

  interference = torch.einsum('ifh,igh->ifg', W_norm, W)
  interference[:, torch.arange(cfg.n_features), torch.arange(cfg.n_features)] = 0

  polysemanticity = torch.linalg.norm(interference, dim=-1).cpu()
  net_interference = (interference**2 * model.feature_probability[:, None, :]).sum(-1).cpu()
  norms = torch.linalg.norm(W, 2, dim=-1).cpu()

  WtW = torch.einsum('sih,soh->sio', W, W).cpu()

  # width = weights[0].cpu()
  # x = torch.cumsum(width+0.1, 0) - width[0]
  x = torch.arange(cfg.n_features)
  width = 0.9

  which_instances = np.arange(cfg.n_instances)[which]
  fig = make_subplots(rows=len(which_instances),
                      cols=2,
                      shared_xaxes=True,
                      vertical_spacing=0.02,
                      horizontal_spacing=0.1)
  for (row, inst) in enumerate(which_instances):
    fig.add_trace(
        go.Bar(x=x, 
              y=norms[inst],
              marker=dict(
                  color=polysemanticity[inst],
                  cmin=0,
                  cmax=1
              ),
              width=width,
        ),
        row=1+row, col=1
    )
    data = WtW[inst].numpy()
    fig.add_trace(
        go.Image(
            z=plt.cm.coolwarm((1 + data)/2, bytes=True),
            colormodel='rgba256',
            customdata=data,
            hovertemplate='''\
In: %{x}<br>
Out: %{y}<br>
Weight: %{customdata:0.2f}
'''            
        ),
        row=1+row, col=2
    )

  fig.add_vline(
    x=(x[cfg.n_hidden-1]+x[cfg.n_hidden])/2, 
    line=dict(width=0.5),
    col=1,
  )
    
  # fig.update_traces(marker_size=1)
  fig.update_layout(showlegend=False, 
                    width=600,
                    height=100*len(which_instances),
                    margin=dict(t=0, b=0))
  fig.update_xaxes(visible=False)
  fig.update_yaxes(visible=False)
  #images_dir = '/content/drive/My Drive/Colab Notebooks/edited_toy_images'
  #curr = time.time()
  #%pip install kaleido
  #fig.write_image(f"{images_dir}/{model.depend}/{curr}.png", engine="kaleido")
  return fig

#!pip install -U kaleido
fig = render_features(model, np.s_[::2])
fig.update_layout()

"""# Viz. Features w/ Dependence"""

config = Config(
    n_features = 20,
    n_hidden = 5,
    n_instances = 7,
)

model_indep = Model(
    config=config,
    device=DEVICE,
    # Exponential feature importance curve from 1 to 1/100
    importance = (100 ** -torch.linspace(0, 1, config.n_features))[None, :],
    # Sweep feature frequency across the instances from 1 (fully dense) to 1/20
    feature_probability = (20 ** -torch.linspace(0, 1, config.n_instances))[:, None],
    depend = -1
)

optimize(model_indep)

fig = render_features(model_indep, np.s_[::1])
fig.update_layout()

config = Config(
    n_features = 20,
    n_hidden = 5,
    n_instances = 7,
)

model_depend_0 = Model(
    config=config,
    device=DEVICE,
    # Exponential feature importance curve from 1 to 1/100
    importance = (100 ** -torch.linspace(0, 1, config.n_features))[None, :],
    # Sweep feature frequency across the instances from 1 (fully dense) to 1/20
    feature_probability = (20 ** -torch.linspace(0, 1, config.n_instances))[:, None],
    depend = 2
)

optimize(model_depend_0)

fig = render_features(model_depend_0, np.s_[::1])
fig.update_layout()

"""# Feature geometry"""

config = Config(
    n_features = 200,
    n_hidden = 20,
    n_instances = 20,
)

model = Model(
    config=config,
    device=DEVICE,
    # For this experiment, use constant importance.

    # Sweep feature frequency across the instances from 1 (fully dense) to 1/20
    feature_probability = (20 ** -torch.linspace(0, 1, config.n_instances))[:, None]
)

optimize(model)

fig = px.line(
    x=1/model.feature_probability[:, 0].cpu(),
    y=(model.config.n_hidden/(torch.linalg.matrix_norm(model.W.detach(), 'fro')**2)).cpu(),
    log_x=True,
    markers=True,
)
fig.update_xaxes(title="1/(1-S)")
fig.update_yaxes(title=f"m/||W||_F^2")

@torch.no_grad()
def compute_dimensionality(W):
  norms = torch.linalg.norm(W, 2, dim=-1) 
  W_unit = W / torch.clamp(norms[:, :, None], 1e-6, float('inf'))

  interferences = (torch.einsum('eah,ebh->eab', W_unit, W)**2).sum(-1)

  dim_fracs = (norms**2/interferences)
  return dim_fracs.cpu()

dim_fracs = compute_dimensionality(model.W)

fig = go.Figure()

density = model.feature_probability[:, 0].cpu()
W = model.W.detach()

for a,b in [(1,2), (2,3), (2,5), (2,6), (2,7)]:
    val = a/b
    fig.add_hline(val, line_color="purple", opacity=0.2, annotation=dict(text=f"{a}/{b}"))

for a,b in [(5,6), (4,5), (3,4), (3,8), (3,12), (3,20)]:
    val = a/b
    fig.add_hline(val, line_color="blue", opacity=0.2, annotation=dict(text=f"{a}/{b}", x=0.05))

for i in range(len(W)):
    fracs_ = dim_fracs[i]
    N = fracs_.shape[0]
    xs = 1/density
    if i!= len(W)-1:
        dx = xs[i+1]-xs[i]
    fig.add_trace(
        go.Scatter(
            x=1/density[i]*np.ones(N)+dx*np.random.uniform(-0.1,0.1,N),
            y=fracs_,
            marker=dict(
                color='black',
                size=1,
                opacity=0.5,
            ),
            mode='markers',
        )
    )

fig.update_xaxes(
    type='log', 
    title='1/(1-S)',
    showgrid=False,
)
fig.update_yaxes(
    showgrid=False
)
fig.update_layout(showlegend=False)